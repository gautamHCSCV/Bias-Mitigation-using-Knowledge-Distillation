{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbb89e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a94298f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "760c4a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77711eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    saved_path=\"saved_models/efficientb4_cifar10.pt\",\n",
    "    best_saved_path = \"saved/random_best.pt\",\n",
    "    lr=0.001, \n",
    "    EPOCHS = 3,\n",
    "    BATCH_SIZE = 32,\n",
    "    IMAGE_SIZE = 132,\n",
    "    TRAIN_VALID_SPLIT = 0.2,\n",
    "    device=device,\n",
    "    SEED = 42,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    USE_AMP = True,\n",
    "    channels_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8776495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(config['SEED'])\n",
    "# If you or any of the libraries you are using rely on NumPy, you can seed the global NumPy RNG \n",
    "np.random.seed(config['SEED'])\n",
    "# Prevent RNG for CPU and GPU using torch\n",
    "torch.manual_seed(config['SEED'])\n",
    "torch.cuda.manual_seed(config['SEED'])\n",
    "torch.backends.cudnn.benchmarks = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e0a06e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop((config['IMAGE_SIZE'],config['IMAGE_SIZE'])),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((config['IMAGE_SIZE'],config['IMAGE_SIZE'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((config['IMAGE_SIZE'],config['IMAGE_SIZE'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18483019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iraq_iran_earthquake',\n",
       " 'srilanka_floods',\n",
       " 'mexico_earthquake',\n",
       " 'hurricane_harvey',\n",
       " 'california_wildfires']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path1 = '../../dataset/CrisisMMD_v2.0/CrisisMMD_v2.0/'\n",
    "classes = os.listdir('../../dataset/CrisisMMD_v2.0/CrisisMMD_v2.0/data_image/')\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fb3b4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Readme.txt',\n",
       " 'task_damage_text_img_dev.tsv',\n",
       " 'task_humanitarian_text_img_train.tsv',\n",
       " 'task_damage_text_img_test.tsv',\n",
       " 'task_informative_text_img_test.tsv',\n",
       " 'task_informative_text_img_train.tsv',\n",
       " 'task_humanitarian_text_img_test.tsv',\n",
       " 'task_damage_text_img_train.tsv',\n",
       " 'task_informative_text_img_dev.tsv',\n",
       " '.ipynb_checkpoints',\n",
       " 'task_humanitarian_text_img_dev.tsv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../../dataset/CrisisMMD_v2.0/CrisisMMD_v2.0/crisismmd_datasplit_all/'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3063d0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2237, 9)\n",
      "(13608, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_name</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "      <th>label_image</th>\n",
       "      <th>label_text_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>917791291823591425_1</td>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>not_humanitarian</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>not_humanitarian</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>917791291823591425_0</td>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917793137925459968</td>\n",
       "      <td>917793137925459968_0</td>\n",
       "      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917793137925459968</td>\n",
       "      <td>917793137925459968_1</td>\n",
       "      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917793137925459968</td>\n",
       "      <td>917793137925459968_2</td>\n",
       "      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             event_name            tweet_id              image_id  \\\n",
       "0  california_wildfires  917791291823591425  917791291823591425_1   \n",
       "1  california_wildfires  917791291823591425  917791291823591425_0   \n",
       "2  california_wildfires  917793137925459968  917793137925459968_0   \n",
       "3  california_wildfires  917793137925459968  917793137925459968_1   \n",
       "4  california_wildfires  917793137925459968  917793137925459968_2   \n",
       "\n",
       "                                          tweet_text  \\\n",
       "0  RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
       "1  RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
       "2  RT @KAKEnews: California wildfires destroy mor...   \n",
       "3  RT @KAKEnews: California wildfires destroy mor...   \n",
       "4  RT @KAKEnews: California wildfires destroy mor...   \n",
       "\n",
       "                                               image  \\\n",
       "0  data_image/california_wildfires/10_10_2017/917...   \n",
       "1  data_image/california_wildfires/10_10_2017/917...   \n",
       "2  data_image/california_wildfires/10_10_2017/917...   \n",
       "3  data_image/california_wildfires/10_10_2017/917...   \n",
       "4  data_image/california_wildfires/10_10_2017/917...   \n",
       "\n",
       "                               label                         label_text  \\\n",
       "0                   not_humanitarian         other_relevant_information   \n",
       "1         other_relevant_information         other_relevant_information   \n",
       "2  infrastructure_and_utility_damage  infrastructure_and_utility_damage   \n",
       "3  infrastructure_and_utility_damage  infrastructure_and_utility_damage   \n",
       "4  infrastructure_and_utility_damage  infrastructure_and_utility_damage   \n",
       "\n",
       "                         label_image label_text_image  \n",
       "0                   not_humanitarian         Negative  \n",
       "1  infrastructure_and_utility_damage         Negative  \n",
       "2  infrastructure_and_utility_damage         Positive  \n",
       "3  infrastructure_and_utility_damage         Positive  \n",
       "4  infrastructure_and_utility_damage         Positive  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path+'task_humanitarian_text_img_train.tsv',sep = '\\t')\n",
    "df_test = pd.read_csv(path+'task_humanitarian_text_img_test.tsv',sep = '\\t')\n",
    "print(df_test.shape)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d009eabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6765, 9)\n",
      "(1160, 9)\n"
     ]
    }
   ],
   "source": [
    "# Filtering out required 5 classes\n",
    "for i in range(len(df)-1,-1,-1):\n",
    "    d = 1\n",
    "    for j in classes:\n",
    "        if j in df['image'][i]:\n",
    "            d = 0\n",
    "            continue\n",
    "    if d: df.drop(i, inplace = True)\n",
    "    \n",
    "    if i<len(df_test):\n",
    "        d = 1\n",
    "        for j in classes:\n",
    "            if j in df_test['image'][i]:\n",
    "                d = 0\n",
    "                continue\n",
    "        if d: df_test.drop(i, inplace = True)\n",
    "print(df.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dde945ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_name</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "      <th>label_image</th>\n",
       "      <th>label_text_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>917791291823591425_1</td>\n",
       "      <td>[2050, 30102, 30108, 2890, 11847, 3748, 10273,...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>not_humanitarian</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>not_humanitarian</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>917791291823591425_0</td>\n",
       "      <td>[2050, 30102, 30108, 2890, 11847, 3748, 10273,...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917793137925459968</td>\n",
       "      <td>917793137925459968_0</td>\n",
       "      <td>[1024, 2662, 3748, 26332, 6033, 2062, 2084, 27...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917793137925459968</td>\n",
       "      <td>917793137925459968_1</td>\n",
       "      <td>[1024, 2662, 3748, 26332, 6033, 2062, 2084, 27...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917793137925459968</td>\n",
       "      <td>917793137925459968_2</td>\n",
       "      <td>[1024, 2662, 3748, 26332, 6033, 2062, 2084, 27...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             event_name            tweet_id              image_id  \\\n",
       "0  california_wildfires  917791291823591425  917791291823591425_1   \n",
       "1  california_wildfires  917791291823591425  917791291823591425_0   \n",
       "2  california_wildfires  917793137925459968  917793137925459968_0   \n",
       "3  california_wildfires  917793137925459968  917793137925459968_1   \n",
       "4  california_wildfires  917793137925459968  917793137925459968_2   \n",
       "\n",
       "                                          tweet_text  \\\n",
       "0  [2050, 30102, 30108, 2890, 11847, 3748, 10273,...   \n",
       "1  [2050, 30102, 30108, 2890, 11847, 3748, 10273,...   \n",
       "2  [1024, 2662, 3748, 26332, 6033, 2062, 2084, 27...   \n",
       "3  [1024, 2662, 3748, 26332, 6033, 2062, 2084, 27...   \n",
       "4  [1024, 2662, 3748, 26332, 6033, 2062, 2084, 27...   \n",
       "\n",
       "                                               image  \\\n",
       "0  data_image/california_wildfires/10_10_2017/917...   \n",
       "1  data_image/california_wildfires/10_10_2017/917...   \n",
       "2  data_image/california_wildfires/10_10_2017/917...   \n",
       "3  data_image/california_wildfires/10_10_2017/917...   \n",
       "4  data_image/california_wildfires/10_10_2017/917...   \n",
       "\n",
       "                               label                         label_text  \\\n",
       "0                   not_humanitarian         other_relevant_information   \n",
       "1         other_relevant_information         other_relevant_information   \n",
       "2  infrastructure_and_utility_damage  infrastructure_and_utility_damage   \n",
       "3  infrastructure_and_utility_damage  infrastructure_and_utility_damage   \n",
       "4  infrastructure_and_utility_damage  infrastructure_and_utility_damage   \n",
       "\n",
       "                         label_image label_text_image  \n",
       "0                   not_humanitarian         Negative  \n",
       "1  infrastructure_and_utility_damage         Negative  \n",
       "2  infrastructure_and_utility_damage         Positive  \n",
       "3  infrastructure_and_utility_damage         Positive  \n",
       "4  infrastructure_and_utility_damage         Positive  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, VisualBertModel\n",
    "# Initialize the VilBERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # Tokenize the text and add special tokens\n",
    "    tokenized_text = tokenizer.encode_plus(text, add_special_tokens=True, max_length=128, truncation=True)\n",
    "    return tokenized_text\n",
    "tokenized = df['tweet_text'].apply(tokenize_text)\n",
    "tokenized_test = df_test['tweet_text'].apply(tokenize_text)\n",
    "\n",
    "def f(x):\n",
    "    x = x['input_ids']\n",
    "    l = len(x)\n",
    "    if l==45:\n",
    "        return x\n",
    "    if l>45:\n",
    "        return x[-45:]\n",
    "    if l<45:\n",
    "        return [0]*(45-l)+x\n",
    "\n",
    "df['tweet_text'] = tokenized.apply(f)\n",
    "df_test['tweet_text'] = tokenized_test.apply(f)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22c8ba31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_name</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "      <th>label_image</th>\n",
       "      <th>label_text_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>917791291823591425_1</td>\n",
       "      <td>[2050, 30102, 30108, 2890, 11847, 3748, 10273,...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>not_humanitarian</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>917791291823591425_0</td>\n",
       "      <td>[2050, 30102, 30108, 2890, 11847, 3748, 10273,...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917793137925459968</td>\n",
       "      <td>917793137925459968_0</td>\n",
       "      <td>[1024, 2662, 3748, 26332, 6033, 2062, 2084, 27...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917793137925459968</td>\n",
       "      <td>917793137925459968_1</td>\n",
       "      <td>[1024, 2662, 3748, 26332, 6033, 2062, 2084, 27...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917793137925459968</td>\n",
       "      <td>917793137925459968_2</td>\n",
       "      <td>[1024, 2662, 3748, 26332, 6033, 2062, 2084, 27...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             event_name            tweet_id              image_id  \\\n",
       "0  california_wildfires  917791291823591425  917791291823591425_1   \n",
       "1  california_wildfires  917791291823591425  917791291823591425_0   \n",
       "2  california_wildfires  917793137925459968  917793137925459968_0   \n",
       "3  california_wildfires  917793137925459968  917793137925459968_1   \n",
       "4  california_wildfires  917793137925459968  917793137925459968_2   \n",
       "\n",
       "                                          tweet_text  \\\n",
       "0  [2050, 30102, 30108, 2890, 11847, 3748, 10273,...   \n",
       "1  [2050, 30102, 30108, 2890, 11847, 3748, 10273,...   \n",
       "2  [1024, 2662, 3748, 26332, 6033, 2062, 2084, 27...   \n",
       "3  [1024, 2662, 3748, 26332, 6033, 2062, 2084, 27...   \n",
       "4  [1024, 2662, 3748, 26332, 6033, 2062, 2084, 27...   \n",
       "\n",
       "                                               image  \\\n",
       "0  data_image/california_wildfires/10_10_2017/917...   \n",
       "1  data_image/california_wildfires/10_10_2017/917...   \n",
       "2  data_image/california_wildfires/10_10_2017/917...   \n",
       "3  data_image/california_wildfires/10_10_2017/917...   \n",
       "4  data_image/california_wildfires/10_10_2017/917...   \n",
       "\n",
       "                               label  label_text  label_image label_text_image  \n",
       "0                   not_humanitarian           5            4         Negative  \n",
       "1         other_relevant_information           5            1         Negative  \n",
       "2  infrastructure_and_utility_damage           1            1         Positive  \n",
       "3  infrastructure_and_utility_damage           1            1         Positive  \n",
       "4  infrastructure_and_utility_damage           1            1         Positive  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "enc.fit(df['label_text'])\n",
    "df['label_text'] = enc.transform(df['label_text'])\n",
    "df_test['label_text'] = enc.transform(df_test['label_text'])\n",
    "\n",
    "\n",
    "enc = LabelEncoder()\n",
    "enc.fit(df['label_image'])\n",
    "df['label_image'] = enc.transform(df['label_image'])\n",
    "df_test['label_image'] = enc.transform(df_test['label_image'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7419aef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6765 1160\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #print(idx)\n",
    "        # Load image from path and convert to tensor\n",
    "        img_path = path1+self.df.iloc[idx]['image']\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_tensor = data_transforms['test'](img)\n",
    "        \n",
    "        # Load text and convert to tensor\n",
    "        text = self.df.iloc[idx]['tweet_text']\n",
    "        text_tensor = torch.tensor(text)\n",
    "        \n",
    "        # Load labels and convert to tensor\n",
    "        img_label = self.df.iloc[idx]['label_image']\n",
    "        img_label_tensor = torch.tensor(img_label)\n",
    "        \n",
    "        text_label = self.df.iloc[idx]['label_text']\n",
    "        text_label_tensor = torch.tensor(text_label)\n",
    "        \n",
    "        return (img_tensor, text_tensor), (img_label_tensor, text_label_tensor)\n",
    "    \n",
    "train_data = CustomDataset(df)\n",
    "test_data = CustomDataset(df_test)\n",
    "valid_data = test_data\n",
    "\n",
    "print(len(train_data), len(valid_data))\n",
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size=32,shuffle=True, num_workers = config['num_workers'],\n",
    "                                          pin_memory = config['pin_memory'])\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=32,shuffle=True, num_workers = config['num_workers'],\n",
    "                                          pin_memory = config['pin_memory'])\n",
    "valid_dl = torch.utils.data.DataLoader(valid_data, batch_size=32,shuffle=True, num_workers = config['num_workers'],\n",
    "                                          pin_memory = config['pin_memory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e3cb5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 132, 132]) torch.Size([32, 45])\n",
      "tensor([4, 6, 4, 1, 4, 4, 1, 4, 4, 0, 1, 1, 1, 4, 4, 0, 5, 1, 4, 4, 4, 6, 0, 4,\n",
      "        4, 4, 4, 4, 1, 6, 6, 4]) tensor([4, 5, 4, 5, 4, 4, 6, 4, 4, 5, 6, 2, 5, 4, 4, 5, 0, 7, 4, 5, 1, 6, 6, 6,\n",
      "        4, 6, 1, 4, 6, 6, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "a = iter(valid_dl)\n",
    "b = next(a)\n",
    "print(b[0][0].shape, b[0][1].shape)\n",
    "print(b[1][0], b[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4281f6d8",
   "metadata": {},
   "source": [
    "# Finetune Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d050053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b0\n",
    "from transformers import BertModel\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_image_classes, num_text_classes):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        \n",
    "        # Define the image classification model using EfficientNet-B0\n",
    "        efficientnet = efficientnet_b0(pretrained=True)\n",
    "        efficientnet.classifier[1] = nn.Linear(in_features = 1280, out_features = num_image_classes, bias = True)\n",
    "        self.image_model = efficientnet\n",
    "        \n",
    "        # Define the text classification model using BERT\n",
    "        self.text_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.text_classifier = nn.Linear(768, num_text_classes)\n",
    "        \n",
    "    def forward(self, img_input, text_input):\n",
    "        # Forward pass for the image classification model\n",
    "        img_output = self.image_model(img_input)\n",
    "        \n",
    "        # Forward pass for the text classification model\n",
    "        text_output = self.text_model(text_input)[1]\n",
    "        text_output = self.text_classifier(text_output)\n",
    "        \n",
    "        return img_output, text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2125cd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = MultiTaskModel(num_image_classes=8, num_text_classes=8)\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss functions for each task\n",
    "img_criterion = nn.CrossEntropyLoss()\n",
    "text_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b19bf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Avg. Image Loss: 1.1108 Avg. Text Loss: 1.6466\n",
      "Image Accuracy: 60.916481892091646  Text Accuracy: 29.652623798965262\n",
      "Validation data:\n",
      "Image Accuracy: 68.44827586206897  Text Accuracy: 32.8448275862069\n",
      "\n",
      "Train Epoch: 2 Avg. Image Loss: 0.8520 Avg. Text Loss: 1.6230\n",
      "Image Accuracy: 69.53436807095343  Text Accuracy: 29.785661492978566\n",
      "Validation data:\n",
      "Image Accuracy: 69.91379310344827  Text Accuracy: 20.43103448275862\n",
      "\n",
      "Train Epoch: 3 Avg. Image Loss: 0.6984 Avg. Text Loss: 1.6175\n",
      "Image Accuracy: 75.28455284552845  Text Accuracy: 29.416112342941613\n",
      "Validation data:\n",
      "Image Accuracy: 67.41379310344827  Text Accuracy: 32.8448275862069\n",
      "\n",
      "Train Epoch: 4 Avg. Image Loss: 0.5734 Avg. Text Loss: 1.6102\n",
      "Image Accuracy: 80.10347376201035  Text Accuracy: 30.49519586104952\n",
      "Validation data:\n",
      "Image Accuracy: 68.1896551724138  Text Accuracy: 32.8448275862069\n",
      "\n",
      "Train Epoch: 5 Avg. Image Loss: 0.4676 Avg. Text Loss: 1.6164\n",
      "Image Accuracy: 82.64597191426459  Text Accuracy: 31.175166297117517\n",
      "Validation data:\n",
      "Image Accuracy: 69.05172413793103  Text Accuracy: 29.655172413793103\n",
      "\n",
      "Train Epoch: 6 Avg. Image Loss: 0.3911 Avg. Text Loss: 1.6052\n",
      "Image Accuracy: 86.20842572062084  Text Accuracy: 30.9830007390983\n",
      "Validation data:\n",
      "Image Accuracy: 66.89655172413794  Text Accuracy: 29.655172413793103\n",
      "\n",
      "Train Epoch: 7 Avg. Image Loss: 0.3410 Avg. Text Loss: 1.6037\n",
      "Image Accuracy: 87.8640059127864  Text Accuracy: 30.89430894308943\n",
      "Validation data:\n",
      "Image Accuracy: 67.84482758620689  Text Accuracy: 29.655172413793103\n",
      "\n",
      "Train Epoch: 8 Avg. Image Loss: 0.2851 Avg. Text Loss: 1.6159\n",
      "Image Accuracy: 89.88913525498891  Text Accuracy: 31.30820399113082\n",
      "Validation data:\n",
      "Image Accuracy: 68.79310344827586  Text Accuracy: 32.8448275862069\n",
      "\n",
      "Train Epoch: 9 Avg. Image Loss: 0.2230 Avg. Text Loss: 1.6088\n",
      "Image Accuracy: 92.40206947524021  Text Accuracy: 30.56910569105691\n",
      "Validation data:\n",
      "Image Accuracy: 68.70689655172414  Text Accuracy: 32.8448275862069\n",
      "\n",
      "Train Epoch: 10 Avg. Image Loss: 0.2111 Avg. Text Loss: 1.6194\n",
      "Image Accuracy: 92.38728750923873  Text Accuracy: 30.08130081300813\n",
      "Validation data:\n",
      "Image Accuracy: 67.75862068965517  Text Accuracy: 32.8448275862069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "def train(model, num_epochs = 10):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        total_img_loss = 0\n",
    "        total_text_loss = 0\n",
    "        total_img_correct = 0\n",
    "        total_text_correct = 0\n",
    "        for batch_idx, batch_data in enumerate(train_dl):\n",
    "            # Get the image and text inputs and labels for this batch\n",
    "            img_inputs = batch_data[0][0].to(device)\n",
    "            text_inputs = batch_data[0][1].to(device)\n",
    "            img_labels = batch_data[1][0].to(device)\n",
    "            text_labels = batch_data[1][1].to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            img_outputs, text_outputs = model(img_inputs, text_inputs)\n",
    "            img_loss = img_criterion(img_outputs, img_labels)\n",
    "            text_loss = text_criterion(text_outputs, text_labels)\n",
    "            loss = img_loss + text_loss\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss for this batch\n",
    "            total_img_loss += img_loss.item()\n",
    "            total_text_loss += text_loss.item()\n",
    "\n",
    "            img_preds = torch.argmax(img_outputs, dim=1)\n",
    "            text_preds = torch.argmax(text_outputs, dim=1)\n",
    "            total_img_correct += (img_preds == img_labels).sum().item()\n",
    "            total_text_correct += (text_preds == text_labels).sum().item()\n",
    "\n",
    "        # Compute the accuracy for this epoch\n",
    "        img_accuracy = 100.0 * total_img_correct / len(train_data)\n",
    "        text_accuracy = 100.0 * total_text_correct / len(train_data)\n",
    "\n",
    "        # Compute the average loss for this epoch\n",
    "        avg_img_loss = total_img_loss / len(train_dl)\n",
    "        avg_text_loss = total_text_loss / len(train_dl)\n",
    "        print(f'Train Epoch: {epoch+1} Avg. Image Loss: {avg_img_loss:.4f} Avg. Text Loss: {avg_text_loss:.4f}')\n",
    "        print(f'Image Accuracy: {img_accuracy}  Text Accuracy: {text_accuracy}')\n",
    "\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        total_img_correct = 0\n",
    "        total_text_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(valid_dl):\n",
    "                # Get the image and text inputs and labels for this batch\n",
    "                img_inputs = batch_data[0][0].to(device)\n",
    "                text_inputs = batch_data[0][1].to(device)\n",
    "                img_labels = batch_data[1][0].to(device)\n",
    "                text_labels = batch_data[1][1].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                img_outputs, text_outputs = model(img_inputs, text_inputs)\n",
    "                img_preds = torch.argmax(img_outputs, dim=1)\n",
    "                text_preds = torch.argmax(text_outputs, dim=1)\n",
    "                total_img_correct += (img_preds == img_labels).sum().item()\n",
    "                total_text_correct += (text_preds == text_labels).sum().item()\n",
    "\n",
    "        # Compute the accuracy for this epoch\n",
    "        img_accuracy = 100.0 * total_img_correct / len(valid_data)\n",
    "        text_accuracy = 100.0 * total_text_correct / len(valid_data)\n",
    "        print(f'Validation data:\\nImage Accuracy: {img_accuracy}  Text Accuracy: {text_accuracy}\\n')\n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79766108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73efcad4",
   "metadata": {},
   "source": [
    "# Training the model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e945acdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class ViLBERT(nn.Module):\n",
    "    def __init__(self, num_image_classes, num_text_classes):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        \n",
    "        # Define the image classification model using EfficientNet-B0\n",
    "        efficientnet = efficientnet_b0(pretrained=False)\n",
    "        efficientnet.classifier[1] = nn.Linear(in_features = 1280, out_features = num_image_classes, bias = True)\n",
    "        self.image_model = efficientnet\n",
    "        \n",
    "        # Define the text classification model using BERT\n",
    "        self.text_model = BertModel\n",
    "        self.text_classifier = nn.Linear(768, num_text_classes)\n",
    "        \n",
    "    def forward(self, img_input, text_input):\n",
    "        # Forward pass for the image classification model\n",
    "        img_output = self.image_model(img_input)\n",
    "        \n",
    "        # Forward pass for the text classification model\n",
    "        text_output = self.text_model(text_input)[1]\n",
    "        text_output = self.text_classifier(text_output)\n",
    "        \n",
    "        return img_output, text_output\n",
    "    \n",
    "    \n",
    "model = MultiTaskModel(num_image_classes=8, num_text_classes=8)\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss functions for each task\n",
    "img_criterion = nn.CrossEntropyLoss()\n",
    "text_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d7e9d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Avg. Image Loss: 1.0997 Avg. Text Loss: 1.7276\n",
      "Image Accuracy: 61.286031042128606  Text Accuracy: 29.534368070953438\n",
      "Validation data:\n",
      "Image Accuracy: 65.43103448275862  Text Accuracy: 32.8448275862069\n",
      "\n",
      "Train Epoch: 2 Avg. Image Loss: 0.8684 Avg. Text Loss: 1.6187\n",
      "Image Accuracy: 68.76570583887657  Text Accuracy: 30.554323725055433\n",
      "Validation data:\n",
      "Image Accuracy: 66.72413793103448  Text Accuracy: 32.8448275862069\n",
      "\n",
      "Train Epoch: 3 Avg. Image Loss: 0.7124 Avg. Text Loss: 1.6073\n",
      "Image Accuracy: 75.24020694752402  Text Accuracy: 29.59349593495935\n",
      "Validation data:\n",
      "Image Accuracy: 70.0  Text Accuracy: 32.8448275862069\n",
      "\n",
      "Train Epoch: 4 Avg. Image Loss: 0.5802 Avg. Text Loss: 1.6071\n",
      "Image Accuracy: 79.58610495195862  Text Accuracy: 30.199556541019955\n",
      "Validation data:\n",
      "Image Accuracy: 68.44827586206897  Text Accuracy: 20.43103448275862\n",
      "\n",
      "Train Epoch: 5 Avg. Image Loss: 0.4756 Avg. Text Loss: 1.6150\n",
      "Image Accuracy: 83.39985218033999  Text Accuracy: 30.066518847006652\n",
      "Validation data:\n",
      "Image Accuracy: 67.06896551724138  Text Accuracy: 32.8448275862069\n",
      "\n",
      "Train Epoch: 6 Avg. Image Loss: 0.3833 Avg. Text Loss: 1.6197\n",
      "Image Accuracy: 86.5779748706578  Text Accuracy: 30.49519586104952\n",
      "Validation data:\n",
      "Image Accuracy: 66.12068965517241  Text Accuracy: 32.8448275862069\n",
      "\n",
      "Train Epoch: 7 Avg. Image Loss: 0.3121 Avg. Text Loss: 1.6253\n",
      "Image Accuracy: 89.04656319290466  Text Accuracy: 29.770879526977087\n",
      "Validation data:\n",
      "Image Accuracy: 66.20689655172414  Text Accuracy: 32.8448275862069\n",
      "\n",
      "Train Epoch: 8 Avg. Image Loss: 0.2837 Avg. Text Loss: 1.6061\n",
      "Image Accuracy: 90.22912047302292  Text Accuracy: 30.051736881005173\n",
      "Validation data:\n",
      "Image Accuracy: 67.93103448275862  Text Accuracy: 32.8448275862069\n",
      "\n",
      "Train Epoch: 9 Avg. Image Loss: 0.2463 Avg. Text Loss: 1.6031\n",
      "Image Accuracy: 91.58906134515891  Text Accuracy: 30.24390243902439\n",
      "Validation data:\n",
      "Image Accuracy: 62.58620689655172  Text Accuracy: 29.655172413793103\n",
      "\n",
      "Train Epoch: 10 Avg. Image Loss: 0.2269 Avg. Text Loss: 1.6125\n",
      "Image Accuracy: 92.1951219512195  Text Accuracy: 29.386548410938655\n",
      "Validation data:\n",
      "Image Accuracy: 68.01724137931035  Text Accuracy: 32.8448275862069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463b167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d18d8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
